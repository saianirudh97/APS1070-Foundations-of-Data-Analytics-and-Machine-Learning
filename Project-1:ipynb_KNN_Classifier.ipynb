{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Project_1.ipynb-KNN Classifier",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08owH-AVyZCX",
        "colab_type": "text"
      },
      "source": [
        "# APS1070\n",
        "#### Basic Principles and Models - Lab 1\n",
        "**Deadline: Jan 24, 23:59 - 10 points**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qonbueFGyemb",
        "colab_type": "text"
      },
      "source": [
        "Name: Sai Anirudh Basamsetty\n",
        "\n",
        "Student ID:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dykrw3cyy7PF",
        "colab_type": "text"
      },
      "source": [
        "##**Marking Scheme:**\n",
        "\n",
        "This project is worth **10 marks** of your final grade.\n",
        "\n",
        "**One (1) mark** of the lab is dedicated to **vectorized coding**. If you need to write a loop in your solution, think about how you can implement the same functionality with vectorized operations. Try to avoid loops as much as possible (in some cases loops are inevitable).\n",
        "\n",
        "This notebook is composed of two sections, a Tutorial, and an Exercise. \n",
        "\n",
        "The TAs in the lab will help you to complete your tutorial (Although no mark is assigned to the **tutorial** compeleting that section is **mandatory**). \n",
        "\n",
        "**The exercise** section is worth **9 points**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwPF0KFQyZCa",
        "colab_type": "text"
      },
      "source": [
        "##Tutorial\n",
        "In this lab, we will be using the popular machine learning library [scikit-learn](https://scikit-learn.org/stable/) in tandem with a popular scientific computing library in Python, [NumPy](https://www.numpy.org/), to investigate basic machine learning principles and models. The topics that will be covered in this lab include:\n",
        "* Introduction to scikit-learn and NumPy\n",
        "* Data preparation and cleaning with Pandas\n",
        "* Exploratory data analysis (EDA)\n",
        "* Nearest neighbors classification algorithm\n",
        "\n",
        "*Note:* Some other useful Python libraries include [matplotlib](https://matplotlib.org/) (for plotting/graphing) and [Pandas](https://pandas.pydata.org/) (for data analysis), though we won't be going into detail on these in this bootcamp. \n",
        "\n",
        "##### Jupyter Notebooks\n",
        "This lab will be using [Jupyter Notebooks](https://jupyter.org/) as a Python development environment. Hopefully you're somewhat familiar with them. Write your code in *cells* (this is a cell!) and execute your code by pressing the *play* button (up top) or by entering *ctrl+enter*. To format a cell for text, you can select \"Markdown\" from the dropdown - the default formatting is \"Code\", which will usually be what you want.\n",
        "\n",
        "#### Getting started\n",
        "Let's get started. First, we're going to test that we're able to import the required libraries.  \n",
        "**>> Run the code in the next cell** to import scikit-learn and NumPy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImUix5ZzyZCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import sklearn "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siJZHN9gyZCl",
        "colab_type": "text"
      },
      "source": [
        "### NumPy Basics\n",
        "\n",
        "Great. Let's move on to our next topic: getting a handle on NumPy basics. You can think of NumPy as sort of like a MATLAB for Python (if that helps). The main object is multidimensional arrays, and these come in particularly handy when working with data and machine learning algorithms.\n",
        "\n",
        "Let's create a 2x4 array containing the numbers 1 through 8 and conduct some basic operations on it.  \n",
        "**>> Run the code in the next cell to create and print the array.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLOnl7TLyZCm",
        "colab_type": "code",
        "outputId": "994bf909-71f0-4d34-e5c7-720d0c8db08b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "array = np.arange(8).reshape(2,4)\n",
        "array"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 2, 3],\n",
              "       [4, 5, 6, 7]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbPOCjNjyZCp",
        "colab_type": "text"
      },
      "source": [
        "We can access the shape, number of dimensions, data type, and number of elements in our array as follows:  \n",
        "*(Tip: use \"print()\" when you want a cell to output more than one thing, or you want to append text to your output, otherwise the cell will output the last object you call, as in the cell above)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShbOGLUHyZCq",
        "colab_type": "code",
        "outputId": "8cecc412-282c-4cc9-982d-dde7345c03e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "print (\"Shape:\", array.shape)\n",
        "print (\"Dimensions:\", array.ndim)\n",
        "print (\"Data type:\" , array.dtype.name)\n",
        "print (\"Number of elements:\", array.size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: (2, 4)\n",
            "Dimensions: 2\n",
            "Data type: int64\n",
            "Number of elements: 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJyUJgrEyZCu",
        "colab_type": "text"
      },
      "source": [
        "If we have a Python list containing a set of numbers, we can use it to create an array:  \n",
        "*(Tip: if you click on a function call, such as array(), and press \"shift+tab\" the Notebook will provide you all the details of the function)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opgqbuJCyZCw",
        "colab_type": "code",
        "outputId": "416fa9d2-8c84-44c2-9985-865e381d037c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "mylist = [0, 1, 1, 2, 3, 5, 8, 13, 21]\n",
        "myarray = np.array(mylist)\n",
        "myarray"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  1,  2,  3,  5,  8, 13, 21])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9aAe1yXyZC1",
        "colab_type": "text"
      },
      "source": [
        "And we can do it for nested lists as well, creating multidimensional NumPy arrays:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE94l8bQyZC2",
        "colab_type": "code",
        "outputId": "503d4c4a-09f6-4165-f114-2372d29fb578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "my2dlist = [[1,2,3],[4,5,6]]\n",
        "my2darray = np.array(my2dlist)\n",
        "my2darray"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2, 3],\n",
              "       [4, 5, 6]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZefTZQ67yZC8",
        "colab_type": "text"
      },
      "source": [
        "We can also index and slice NumPy arrays like we would do with a Python list or another container object as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-dkbQpCyZC9",
        "colab_type": "code",
        "outputId": "6e2b9baa-8f17-45ed-cf78-2db75d880614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "array = np.arange(10)\n",
        "print (\"Originally: \", array)\n",
        "print (\"First four elements: \", array[:4])\n",
        "print (\"After the first four elements: \", array[4:])\n",
        "print (\"The last element: \", array[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Originally:  [0 1 2 3 4 5 6 7 8 9]\n",
            "First four elements:  [0 1 2 3]\n",
            "After the first four elements:  [4 5 6 7 8 9]\n",
            "The last element:  9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhN9DA7myZDA",
        "colab_type": "text"
      },
      "source": [
        "And we can index/slice multidimensional arrays, too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_BlSFH1yZDB",
        "colab_type": "code",
        "outputId": "6096c9d6-3ae0-4e40-d9d0-a4fd89857abf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "array = np.array([[1,2,3],[4,5,6]])\n",
        "print (\"Originally: \", array)\n",
        "print (\"First row only: \", array[0,:])\n",
        "print (\"First column only: \", array[:,0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Originally:  [[1 2 3]\n",
            " [4 5 6]]\n",
            "First row only:  [1 2 3]\n",
            "First column only:  [1 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhClSzJpyZDE",
        "colab_type": "text"
      },
      "source": [
        "#### Sneak preview\n",
        "\n",
        "Often, when designing a machine learning classifier, it can be useful to compare an array of predictions (0 or 1 values) to another array of true values. We can do this pretty easily in NumPy to compute the *accuracy* (e.g., the number of values that are the same), for example, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3MUxHymyZDG",
        "colab_type": "code",
        "outputId": "0d2f0959-c279-4175-a431-2f973328c280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "true_values = [0, 0, 1, 1, 1, 1, 1, 0, 1, 0]\n",
        "predictions = [0, 0, 0, 1, 1, 1, 0, 1, 1, 0]\n",
        "\n",
        "true_values_array = np.array(true_values)\n",
        "predictions_array = np.array(predictions)\n",
        "\n",
        "accuracy = np.sum(true_values_array == predictions_array) / true_values_array.size\n",
        "print (\"Accuracy: \", accuracy * 100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  70.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGkUlHdSyZDJ",
        "colab_type": "text"
      },
      "source": [
        "In the previous cell, we took two Python lists, converted them to NumPy arrays, and then used a combination of np.sum() and .size to compute the accuracy (proportion of elements that are pairwise equal). A tiny bit more advanced, but demonstrates the power of NumPy arrays.\n",
        "\n",
        "You'll notice we didn't used nested loops to conduct the comparison, but instead used the np.sum() function. This is an example of a vectorized operation within NumPy that is much more efficient when dealing with large datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZIUpzElyZDK",
        "colab_type": "text"
      },
      "source": [
        "### Pandas basics\n",
        "\n",
        "Pandas is an incredibly useful library that allows us to work with large datasets in Python. It contains myriad useful tools, and is highly compatible with other libraries like Scikit-learn, so you don't have to spend any time getting the two to play nicely together.\n",
        "\n",
        "First we are going to load a dataset with Pandas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8kZVXRryZDL",
        "colab_type": "code",
        "outputId": "6e55364d-0a45-47c0-9722-3b81956bb531",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=6257b97caedd8a84bd5b4d48a4e8c6b4f9dc2d7a298bc5645e8039712cd06953\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75PRsoy0yZDO",
        "colab_type": "code",
        "outputId": "381d7a22-0dfe-4115-c8dc-9386ca1b3f59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import wget\n",
        "\n",
        "wget.download(\n",
        "    'https://github.com/alexwolson/APS1070_data/raw/master/arabica_data.csv',\n",
        "    'arabica_data.csv'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'arabica_data.csv'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13FXW71FyZDQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('arabica_data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGUy-OIqyZDT",
        "colab_type": "text"
      },
      "source": [
        "With Pandas, the main object we work with is referred to as a _DataFrame_ (hence calling our object here df). A DataFrame stores our dataset in a way that immediately gives us a lot of power to interact with it. If you just put the DataFrame in a cell on its own, you instantly get a clear, easy to read preview of the data you have:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzm8z_1uyZDU",
        "colab_type": "code",
        "outputId": "780bca4f-f965-4269-8bce-ae3368397b38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Acidity</th>\n",
              "      <th>Aftertaste</th>\n",
              "      <th>Aroma</th>\n",
              "      <th>Bag Weight</th>\n",
              "      <th>Balance</th>\n",
              "      <th>Body</th>\n",
              "      <th>Category.One.Defects</th>\n",
              "      <th>Category.Two.Defects</th>\n",
              "      <th>Clean Cup</th>\n",
              "      <th>Color</th>\n",
              "      <th>Company</th>\n",
              "      <th>Country of Origin</th>\n",
              "      <th>Cupper Points</th>\n",
              "      <th>Expiration</th>\n",
              "      <th>Farm Name</th>\n",
              "      <th>Flavor</th>\n",
              "      <th>Grading Date</th>\n",
              "      <th>Harvest Year</th>\n",
              "      <th>ICO Number</th>\n",
              "      <th>In-Country Partner</th>\n",
              "      <th>Mill</th>\n",
              "      <th>Moisture</th>\n",
              "      <th>Number of Bags</th>\n",
              "      <th>Processing Method</th>\n",
              "      <th>Producer</th>\n",
              "      <th>Region</th>\n",
              "      <th>Species</th>\n",
              "      <th>Sweetness</th>\n",
              "      <th>Uniformity</th>\n",
              "      <th>Variety</th>\n",
              "      <th>altitude_high_meters</th>\n",
              "      <th>altitude_low_meters</th>\n",
              "      <th>altitude_mean_meters</th>\n",
              "      <th>quality_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>8.75</td>\n",
              "      <td>8.67</td>\n",
              "      <td>8.67</td>\n",
              "      <td>60 kg</td>\n",
              "      <td>8.42</td>\n",
              "      <td>8.50</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10.00</td>\n",
              "      <td>Green</td>\n",
              "      <td>metad agricultural developmet plc</td>\n",
              "      <td>Ethiopia</td>\n",
              "      <td>8.75</td>\n",
              "      <td>April 3rd, 2016</td>\n",
              "      <td>METAD PLC</td>\n",
              "      <td>8.83</td>\n",
              "      <td>April 4th, 2015</td>\n",
              "      <td>2014</td>\n",
              "      <td>2014/2015</td>\n",
              "      <td>METAD Agricultural Development plc</td>\n",
              "      <td>metad plc</td>\n",
              "      <td>0.12</td>\n",
              "      <td>300</td>\n",
              "      <td>Washed / Wet</td>\n",
              "      <td>METAD PLC</td>\n",
              "      <td>guji-hambela</td>\n",
              "      <td>Arabica</td>\n",
              "      <td>10.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2200.00</td>\n",
              "      <td>1950.00</td>\n",
              "      <td>2075.00</td>\n",
              "      <td>90.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>8.58</td>\n",
              "      <td>8.50</td>\n",
              "      <td>8.75</td>\n",
              "      <td>60 kg</td>\n",
              "      <td>8.42</td>\n",
              "      <td>8.42</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>10.00</td>\n",
              "      <td>Green</td>\n",
              "      <td>metad agricultural developmet plc</td>\n",
              "      <td>Ethiopia</td>\n",
              "      <td>8.58</td>\n",
              "      <td>April 3rd, 2016</td>\n",
              "      <td>METAD PLC</td>\n",
              "      <td>8.67</td>\n",
              "      <td>April 4th, 2015</td>\n",
              "      <td>2014</td>\n",
              "      <td>2014/2015</td>\n",
              "      <td>METAD Agricultural Development plc</td>\n",
              "      <td>metad plc</td>\n",
              "      <td>0.12</td>\n",
              "      <td>300</td>\n",
              "      <td>Washed / Wet</td>\n",
              "      <td>METAD PLC</td>\n",
              "      <td>guji-hambela</td>\n",
              "      <td>Arabica</td>\n",
              "      <td>10.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>Other</td>\n",
              "      <td>2200.00</td>\n",
              "      <td>1950.00</td>\n",
              "      <td>2075.00</td>\n",
              "      <td>89.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>8.42</td>\n",
              "      <td>8.42</td>\n",
              "      <td>8.42</td>\n",
              "      <td>1</td>\n",
              "      <td>8.42</td>\n",
              "      <td>8.33</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Guatemala</td>\n",
              "      <td>9.25</td>\n",
              "      <td>May 31st, 2011</td>\n",
              "      <td>San Marcos Barrancas \"San Cristobal Cuch</td>\n",
              "      <td>8.50</td>\n",
              "      <td>May 31st, 2010</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Specialty Coffee Association</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.00</td>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Arabica</td>\n",
              "      <td>10.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>Bourbon</td>\n",
              "      <td>1800.00</td>\n",
              "      <td>1600.00</td>\n",
              "      <td>1700.00</td>\n",
              "      <td>89.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>8.42</td>\n",
              "      <td>8.42</td>\n",
              "      <td>8.17</td>\n",
              "      <td>60 kg</td>\n",
              "      <td>8.25</td>\n",
              "      <td>8.50</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>10.00</td>\n",
              "      <td>Green</td>\n",
              "      <td>yidnekachew debessa coffee plantation</td>\n",
              "      <td>Ethiopia</td>\n",
              "      <td>8.67</td>\n",
              "      <td>March 25th, 2016</td>\n",
              "      <td>Yidnekachew Dabessa Coffee Plantation</td>\n",
              "      <td>8.58</td>\n",
              "      <td>March 26th, 2015</td>\n",
              "      <td>2014</td>\n",
              "      <td>NaN</td>\n",
              "      <td>METAD Agricultural Development plc</td>\n",
              "      <td>wolensu</td>\n",
              "      <td>0.11</td>\n",
              "      <td>320</td>\n",
              "      <td>Natural / Dry</td>\n",
              "      <td>Yidnekachew Dabessa Coffee Plantation</td>\n",
              "      <td>oromia</td>\n",
              "      <td>Arabica</td>\n",
              "      <td>10.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2200.00</td>\n",
              "      <td>1800.00</td>\n",
              "      <td>2000.00</td>\n",
              "      <td>89.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>8.50</td>\n",
              "      <td>8.25</td>\n",
              "      <td>8.25</td>\n",
              "      <td>60 kg</td>\n",
              "      <td>8.33</td>\n",
              "      <td>8.42</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>10.00</td>\n",
              "      <td>Green</td>\n",
              "      <td>metad agricultural developmet plc</td>\n",
              "      <td>Ethiopia</td>\n",
              "      <td>8.58</td>\n",
              "      <td>April 3rd, 2016</td>\n",
              "      <td>METAD PLC</td>\n",
              "      <td>8.50</td>\n",
              "      <td>April 4th, 2015</td>\n",
              "      <td>2014</td>\n",
              "      <td>2014/2015</td>\n",
              "      <td>METAD Agricultural Development plc</td>\n",
              "      <td>metad plc</td>\n",
              "      <td>0.12</td>\n",
              "      <td>300</td>\n",
              "      <td>Washed / Wet</td>\n",
              "      <td>METAD PLC</td>\n",
              "      <td>guji-hambela</td>\n",
              "      <td>Arabica</td>\n",
              "      <td>10.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>Other</td>\n",
              "      <td>2200.00</td>\n",
              "      <td>1950.00</td>\n",
              "      <td>2075.00</td>\n",
              "      <td>88.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1306</th>\n",
              "      <td>1306</td>\n",
              "      <td>6.50</td>\n",
              "      <td>6.17</td>\n",
              "      <td>7.00</td>\n",
              "      <td>1 kg</td>\n",
              "      <td>6.17</td>\n",
              "      <td>6.67</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0.00</td>\n",
              "      <td>Green</td>\n",
              "      <td>cadexsa</td>\n",
              "      <td>Mexico</td>\n",
              "      <td>6.75</td>\n",
              "      <td>May 15th, 2015</td>\n",
              "      <td>EL CENTENARIO</td>\n",
              "      <td>6.33</td>\n",
              "      <td>September 17th, 2012</td>\n",
              "      <td>2012</td>\n",
              "      <td>1104328663</td>\n",
              "      <td>AMECAFE</td>\n",
              "      <td>cadexsa</td>\n",
              "      <td>0.10</td>\n",
              "      <td>12</td>\n",
              "      <td>Washed / Wet</td>\n",
              "      <td>Omar Acosta</td>\n",
              "      <td>marcala</td>\n",
              "      <td>Arabica</td>\n",
              "      <td>8.00</td>\n",
              "      <td>8.00</td>\n",
              "      <td>Catuai</td>\n",
              "      <td>1450.00</td>\n",
              "      <td>1450.00</td>\n",
              "      <td>1450.00</td>\n",
              "      <td>68.33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1307</th>\n",
              "      <td>1307</td>\n",
              "      <td>7.42</td>\n",
              "      <td>6.25</td>\n",
              "      <td>7.08</td>\n",
              "      <td>2 kg</td>\n",
              "      <td>6.75</td>\n",
              "      <td>7.25</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>6.00</td>\n",
              "      <td>None</td>\n",
              "      <td>terra mia</td>\n",
              "      <td>Haiti</td>\n",
              "      <td>6.42</td>\n",
              "      <td>September 17th, 2013</td>\n",
              "      <td>200 farms</td>\n",
              "      <td>6.83</td>\n",
              "      <td>May 24th, 2012</td>\n",
              "      <td>2012</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Specialty Coffee Association</td>\n",
              "      <td>la esperanza, municipio juchique de ferrer, ve...</td>\n",
              "      <td>0.11</td>\n",
              "      <td>1</td>\n",
              "      <td>Natural / Dry</td>\n",
              "      <td>JUAN CARLOS GARCÍA LOPEZ</td>\n",
              "      <td>juchique de ferrer</td>\n",
              "      <td>Arabica</td>\n",
              "      <td>10.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>Bourbon</td>\n",
              "      <td>900.00</td>\n",
              "      <td>900.00</td>\n",
              "      <td>900.00</td>\n",
              "      <td>67.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1308</th>\n",
              "      <td>1308</td>\n",
              "      <td>6.67</td>\n",
              "      <td>6.42</td>\n",
              "      <td>6.75</td>\n",
              "      <td>69 kg</td>\n",
              "      <td>6.67</td>\n",
              "      <td>7.08</td>\n",
              "      <td>8</td>\n",
              "      <td>16</td>\n",
              "      <td>6.00</td>\n",
              "      <td>Blue-Green</td>\n",
              "      <td>haiti coffee</td>\n",
              "      <td>Nicaragua</td>\n",
              "      <td>6.17</td>\n",
              "      <td>May 24th, 2013</td>\n",
              "      <td>Finca Las Marías</td>\n",
              "      <td>6.58</td>\n",
              "      <td>June 6th, 2017</td>\n",
              "      <td>2016</td>\n",
              "      <td>017-053-0211/ 017-053-0212</td>\n",
              "      <td>Instituto Hondureño del Café</td>\n",
              "      <td>coeb koperativ ekselsyo basen (350 members)</td>\n",
              "      <td>0.14</td>\n",
              "      <td>550</td>\n",
              "      <td>Other</td>\n",
              "      <td>COEB Koperativ Ekselsyo Basen</td>\n",
              "      <td>department d'artibonite , haiti</td>\n",
              "      <td>Arabica</td>\n",
              "      <td>6.00</td>\n",
              "      <td>9.33</td>\n",
              "      <td>Typica</td>\n",
              "      <td>350.00</td>\n",
              "      <td>350.00</td>\n",
              "      <td>350.00</td>\n",
              "      <td>63.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1309</th>\n",
              "      <td>1309</td>\n",
              "      <td>6.25</td>\n",
              "      <td>6.33</td>\n",
              "      <td>7.25</td>\n",
              "      <td>1 kg</td>\n",
              "      <td>6.08</td>\n",
              "      <td>6.42</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1.33</td>\n",
              "      <td>Green</td>\n",
              "      <td>exportadora atlantic s.a</td>\n",
              "      <td>Guatemala</td>\n",
              "      <td>6.67</td>\n",
              "      <td>June 6th, 2018</td>\n",
              "      <td>FINCA EL LIMON</td>\n",
              "      <td>6.58</td>\n",
              "      <td>May 24th, 2012</td>\n",
              "      <td>2012</td>\n",
              "      <td>11/853/165</td>\n",
              "      <td>Asociacion Nacional Del Café</td>\n",
              "      <td>beneficio atlantic condega</td>\n",
              "      <td>0.13</td>\n",
              "      <td>275</td>\n",
              "      <td>Washed / Wet</td>\n",
              "      <td>Teófilo Narváez</td>\n",
              "      <td>jalapa</td>\n",
              "      <td>Arabica</td>\n",
              "      <td>6.00</td>\n",
              "      <td>6.00</td>\n",
              "      <td>Caturra</td>\n",
              "      <td>1100.00</td>\n",
              "      <td>1100.00</td>\n",
              "      <td>1100.00</td>\n",
              "      <td>59.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1310</th>\n",
              "      <td>1310</td>\n",
              "      <td>7.67</td>\n",
              "      <td>6.67</td>\n",
              "      <td>7.50</td>\n",
              "      <td>0 lbs</td>\n",
              "      <td>6.67</td>\n",
              "      <td>7.33</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0.00</td>\n",
              "      <td>Green</td>\n",
              "      <td>unicafe</td>\n",
              "      <td>Ethiopia</td>\n",
              "      <td>6.00</td>\n",
              "      <td>May 24th, 2013</td>\n",
              "      <td>TEST</td>\n",
              "      <td>6.67</td>\n",
              "      <td>June 18th, 2010</td>\n",
              "      <td>TEST</td>\n",
              "      <td>TEST</td>\n",
              "      <td>Ethiopia Commodity Exchange</td>\n",
              "      <td>beneficio serben</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WILLIAM ESTUARDO MARTINEZ PACHECO</td>\n",
              "      <td>nuevo oriente</td>\n",
              "      <td>Arabica</td>\n",
              "      <td>1.33</td>\n",
              "      <td>8.00</td>\n",
              "      <td>Catuai</td>\n",
              "      <td>1417.32</td>\n",
              "      <td>1417.32</td>\n",
              "      <td>1417.32</td>\n",
              "      <td>43.13</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1311 rows × 35 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0  Acidity  ...  altitude_mean_meters  quality_score\n",
              "0              0     8.75  ...               2075.00          90.58\n",
              "1              1     8.58  ...               2075.00          89.92\n",
              "2              2     8.42  ...               1700.00          89.75\n",
              "3              3     8.42  ...               2000.00          89.00\n",
              "4              4     8.50  ...               2075.00          88.83\n",
              "...          ...      ...  ...                   ...            ...\n",
              "1306        1306     6.50  ...               1450.00          68.33\n",
              "1307        1307     7.42  ...                900.00          67.92\n",
              "1308        1308     6.67  ...                350.00          63.08\n",
              "1309        1309     6.25  ...               1100.00          59.83\n",
              "1310        1310     7.67  ...               1417.32          43.13\n",
              "\n",
              "[1311 rows x 35 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM8MYtTNyZDX",
        "colab_type": "text"
      },
      "source": [
        "But even though this is printed out well, the dataset is a bit too large for this view to be anything but overwhelming. Luckily, Pandas allows us to easily get some summary statistics about our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zxPSSBIyZDY",
        "colab_type": "code",
        "outputId": "86a45afa-ef15-47f9-a098-70d50242434f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        }
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Acidity</th>\n",
              "      <th>Aftertaste</th>\n",
              "      <th>Aroma</th>\n",
              "      <th>Balance</th>\n",
              "      <th>Body</th>\n",
              "      <th>Category.One.Defects</th>\n",
              "      <th>Category.Two.Defects</th>\n",
              "      <th>Clean Cup</th>\n",
              "      <th>Cupper Points</th>\n",
              "      <th>Flavor</th>\n",
              "      <th>Moisture</th>\n",
              "      <th>Number of Bags</th>\n",
              "      <th>Sweetness</th>\n",
              "      <th>Uniformity</th>\n",
              "      <th>altitude_high_meters</th>\n",
              "      <th>altitude_low_meters</th>\n",
              "      <th>altitude_mean_meters</th>\n",
              "      <th>quality_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1311.000000</td>\n",
              "      <td>1311.000000</td>\n",
              "      <td>1311.000000</td>\n",
              "      <td>1311.000000</td>\n",
              "      <td>1311.000000</td>\n",
              "      <td>1311.000000</td>\n",
              "      <td>1311.000000</td>\n",
              "      <td>1311.000000</td>\n",
              "      <td>1311.00000</td>\n",
              "      <td>1311.000000</td>\n",
              "      <td>1311.000000</td>\n",
              "      <td>1311.000000</td>\n",
              "      <td>1311.000000</td>\n",
              "      <td>1311.000000</td>\n",
              "      <td>1311.000000</td>\n",
              "      <td>1084.000000</td>\n",
              "      <td>1084.000000</td>\n",
              "      <td>1084.000000</td>\n",
              "      <td>1311.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>655.000000</td>\n",
              "      <td>7.538764</td>\n",
              "      <td>7.403158</td>\n",
              "      <td>7.569527</td>\n",
              "      <td>7.523288</td>\n",
              "      <td>7.523387</td>\n",
              "      <td>0.450038</td>\n",
              "      <td>3.626240</td>\n",
              "      <td>9.83312</td>\n",
              "      <td>7.502441</td>\n",
              "      <td>7.523539</td>\n",
              "      <td>0.088963</td>\n",
              "      <td>153.678108</td>\n",
              "      <td>9.910900</td>\n",
              "      <td>9.839497</td>\n",
              "      <td>1808.751552</td>\n",
              "      <td>1759.456703</td>\n",
              "      <td>1784.104128</td>\n",
              "      <td>82.148825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>378.597412</td>\n",
              "      <td>0.319773</td>\n",
              "      <td>0.349945</td>\n",
              "      <td>0.315930</td>\n",
              "      <td>0.349174</td>\n",
              "      <td>0.293089</td>\n",
              "      <td>2.017571</td>\n",
              "      <td>5.482857</td>\n",
              "      <td>0.77135</td>\n",
              "      <td>0.428989</td>\n",
              "      <td>0.341817</td>\n",
              "      <td>0.047907</td>\n",
              "      <td>129.760079</td>\n",
              "      <td>0.454824</td>\n",
              "      <td>0.491508</td>\n",
              "      <td>8767.192330</td>\n",
              "      <td>8767.851565</td>\n",
              "      <td>8767.021485</td>\n",
              "      <td>2.893505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.250000</td>\n",
              "      <td>6.170000</td>\n",
              "      <td>5.080000</td>\n",
              "      <td>6.080000</td>\n",
              "      <td>5.250000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>5.170000</td>\n",
              "      <td>6.080000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.330000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>43.130000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>327.500000</td>\n",
              "      <td>7.330000</td>\n",
              "      <td>7.250000</td>\n",
              "      <td>7.420000</td>\n",
              "      <td>7.330000</td>\n",
              "      <td>7.330000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>7.250000</td>\n",
              "      <td>7.330000</td>\n",
              "      <td>0.090000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>1100.000000</td>\n",
              "      <td>1100.000000</td>\n",
              "      <td>1100.000000</td>\n",
              "      <td>81.170000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>655.000000</td>\n",
              "      <td>7.500000</td>\n",
              "      <td>7.420000</td>\n",
              "      <td>7.580000</td>\n",
              "      <td>7.500000</td>\n",
              "      <td>7.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>7.500000</td>\n",
              "      <td>7.580000</td>\n",
              "      <td>0.110000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>1350.000000</td>\n",
              "      <td>1310.640000</td>\n",
              "      <td>1310.640000</td>\n",
              "      <td>82.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>982.500000</td>\n",
              "      <td>7.750000</td>\n",
              "      <td>7.580000</td>\n",
              "      <td>7.750000</td>\n",
              "      <td>7.750000</td>\n",
              "      <td>7.670000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>7.750000</td>\n",
              "      <td>7.750000</td>\n",
              "      <td>0.120000</td>\n",
              "      <td>275.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>1650.000000</td>\n",
              "      <td>1600.000000</td>\n",
              "      <td>1600.000000</td>\n",
              "      <td>83.670000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1310.000000</td>\n",
              "      <td>8.750000</td>\n",
              "      <td>8.670000</td>\n",
              "      <td>8.750000</td>\n",
              "      <td>8.750000</td>\n",
              "      <td>8.580000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>8.830000</td>\n",
              "      <td>0.280000</td>\n",
              "      <td>1062.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>190164.000000</td>\n",
              "      <td>190164.000000</td>\n",
              "      <td>190164.000000</td>\n",
              "      <td>90.580000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Unnamed: 0      Acidity  ...  altitude_mean_meters  quality_score\n",
              "count  1311.000000  1311.000000  ...           1084.000000    1311.000000\n",
              "mean    655.000000     7.538764  ...           1784.104128      82.148825\n",
              "std     378.597412     0.319773  ...           8767.021485       2.893505\n",
              "min       0.000000     5.250000  ...              1.000000      43.130000\n",
              "25%     327.500000     7.330000  ...           1100.000000      81.170000\n",
              "50%     655.000000     7.500000  ...           1310.640000      82.500000\n",
              "75%     982.500000     7.750000  ...           1600.000000      83.670000\n",
              "max    1310.000000     8.750000  ...         190164.000000      90.580000\n",
              "\n",
              "[8 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr5RndjlyZDb",
        "colab_type": "text"
      },
      "source": [
        "Let's say we want to zero in on a single column. This is done the same way that you access a dictionary entry:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amOkMl4JyZDc",
        "colab_type": "code",
        "outputId": "30b786b5-a7b0-4f07-9be3-c5e38360f056",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "df['Species']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       Arabica\n",
              "1       Arabica\n",
              "2       Arabica\n",
              "3       Arabica\n",
              "4       Arabica\n",
              "         ...   \n",
              "1306    Arabica\n",
              "1307    Arabica\n",
              "1308    Arabica\n",
              "1309    Arabica\n",
              "1310    Arabica\n",
              "Name: Species, Length: 1311, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6aHwWANyZDf",
        "colab_type": "text"
      },
      "source": [
        "Using this method of column access on its own returns a `series` object - think of this as a DataFrame with only one column. If you want to get the raw values however, you can simply specify this by adding `.values` after your entry. Using this, and by putting the object in a `Set` (which does not allow duplicate entries), we can quickly see all of the possible values for any column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXgYAiucyZDf",
        "colab_type": "code",
        "outputId": "c3011cb5-d873-4588-ea5f-a0a87159556c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "source": [
        "set(df['Variety'].values)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Arusha',\n",
              " 'Blue Mountain',\n",
              " 'Bourbon',\n",
              " 'Catimor',\n",
              " 'Catuai',\n",
              " 'Caturra',\n",
              " 'Ethiopian Heirlooms',\n",
              " 'Ethiopian Yirgacheffe',\n",
              " 'Gesha',\n",
              " 'Hawaiian Kona',\n",
              " 'Java',\n",
              " 'Mandheling',\n",
              " 'Marigojipe',\n",
              " 'Moka Peaberry',\n",
              " 'Mundo Novo',\n",
              " 'Other',\n",
              " 'Pacamara',\n",
              " 'Pacas',\n",
              " 'Pache Comun',\n",
              " 'Peaberry',\n",
              " 'Ruiru 11',\n",
              " 'SL14',\n",
              " 'SL28',\n",
              " 'SL34',\n",
              " 'Sulawesi',\n",
              " 'Sumatra',\n",
              " 'Sumatra Lintong',\n",
              " 'Typica',\n",
              " 'Yellow Bourbon',\n",
              " nan}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DceE7LJeyZDj",
        "colab_type": "text"
      },
      "source": [
        "You may notice that the final entry in this set isn't like the others - it's `nan`, which in Pandas denotes a missing entry. When working with real world datasets it's very common for entries to be missing, and there are a variety of ways of approaching a problem like this. For now, though, we are simply going to tell Pandas to drop any row that has a missing column, using the `dropna()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1nh1-ClyZDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_clean = df.dropna()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmFA7BOsyZDv",
        "colab_type": "text"
      },
      "source": [
        "**YOUR TURN** How many entries did we lose by dropping all `nan`s? \n",
        "770\n",
        "\n",
        "* What percentage of entries are left in `df_clean`? \n",
        "41.2662090076278%\n",
        "* What column had the highest number of `nan` entries? (This can be done in one line - use Google!) \n",
        "Farm Name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftS-yaWByZDx",
        "colab_type": "code",
        "outputId": "145c637c-8546-420e-e7da-aaa9f2927298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "### Your code here\n",
        "print(len(df)-len(df_clean))\n",
        "percentage=len(df_clean)/len(df)*100\n",
        "print(percentage,\"%\")\n",
        "p=df.isnull().mean()\n",
        "df.isna().sum() \n",
        "print(df.count().idxmin())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "770\n",
            "41.26620900076278 %\n",
            "Farm Name\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZmx1a6kyZD0",
        "colab_type": "text"
      },
      "source": [
        "As you perform this analysis, you will probably notice that we've lost _quite a bit_ of our original data by simply dropping the `nan` values. There is another approach that we can examine, however. Instead of dropping the missing entries entirely, we can _impute_ their value using the data we do have. For a single column we can do this like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "978WpswmyZD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imp = SimpleImputer(\n",
        "    missing_values=np.nan,\n",
        "    strategy='mean',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "imp.fit(\n",
        "    df['altitude_mean_meters'].values.reshape((-1,1)) #we have to do the reshape operation because we are only using one feature.\n",
        ")\n",
        "\n",
        "df['altitude_mean_meters_imputed'] = imp.transform(df['altitude_mean_meters'].values.reshape((-1,1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmFHt0myyZD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[['altitude_mean_meters','altitude_mean_meters_imputed']].head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvtZNbdoyZD5",
        "colab_type": "text"
      },
      "source": [
        "OK, great! Now we have replaced the useless NaN values with the average height. While this obviously isn't as good as original data, in a lot of situations this can be a step up from losing rows entirely. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wZRacbpyZD7",
        "colab_type": "text"
      },
      "source": [
        "Sophisticated analysis can be done in only a few lines using Pandas. Let's say that we want to get the average coffee rating by country. First, we can use the `groupby` method to automatically collect the results by country. Then, we can select the column we want - `quality_score` - and calculate its mean the same way we would using NumPy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DQSepevyZD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_clean.groupby('Country of Origin')['quality_score'].mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj9yPF4MyZD_",
        "colab_type": "text"
      },
      "source": [
        "This is certainly interesting, but it could be presented better. First, all of the ratings are pretty high (what's the highest and lowest rating?). Let's standardize to unit mean and variance so that we can tell the difference more easily. We'll just do that on our subset here for now, but you can apply it to the entire dataset too!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GerXlhyhyZEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "country_means = df_clean.groupby('Country of Origin')['quality_score'].mean()\n",
        "mu,si = country_means.mean(), country_means.std() #Calculate the overall mean and standard deviation of the quality scores\n",
        "country_means -= mu #Subtract the mean from every entry\n",
        "country_means /= si #Divide every entry by the standard deviation\n",
        "country_means"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH5x0jMtyZEC",
        "colab_type": "text"
      },
      "source": [
        "This is a lot clearer! Finally, let's sort this list so that it's easier to compare entries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lo_Kuq-MyZED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "country_means.sort_values()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPPkd2dUyZEF",
        "colab_type": "text"
      },
      "source": [
        "Finally, we'll look at indexing using Pandas. Let's say that we want to only look at the coffee entries from Taiwan. We can use the following syntax to identify those rows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaiB3FWiyZEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_clean[df_clean['Country of Origin'] == 'Taiwan']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD6HWUV8yZEI",
        "colab_type": "text"
      },
      "source": [
        "Say that out of the Taiwanese coffees, we only want to look at those which are the Bourbon variety. We can also chain those indexing operations like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8P1V8rIyZEJ",
        "colab_type": "code",
        "outputId": "e75aa354-5c03-471d-a6d5-49581dfac8c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "source": [
        "df_clean[df_clean['Country of Origin'] == 'Taiwan'][df_clean['Variety'] == 'Bourbon']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Acidity</th>\n",
              "      <th>Aftertaste</th>\n",
              "      <th>Aroma</th>\n",
              "      <th>Bag Weight</th>\n",
              "      <th>Balance</th>\n",
              "      <th>Body</th>\n",
              "      <th>Category.One.Defects</th>\n",
              "      <th>Category.Two.Defects</th>\n",
              "      <th>Clean Cup</th>\n",
              "      <th>Color</th>\n",
              "      <th>Company</th>\n",
              "      <th>Country of Origin</th>\n",
              "      <th>Cupper Points</th>\n",
              "      <th>Expiration</th>\n",
              "      <th>Farm Name</th>\n",
              "      <th>Flavor</th>\n",
              "      <th>Grading Date</th>\n",
              "      <th>Harvest Year</th>\n",
              "      <th>ICO Number</th>\n",
              "      <th>In-Country Partner</th>\n",
              "      <th>Mill</th>\n",
              "      <th>Moisture</th>\n",
              "      <th>Number of Bags</th>\n",
              "      <th>Processing Method</th>\n",
              "      <th>Producer</th>\n",
              "      <th>Region</th>\n",
              "      <th>Species</th>\n",
              "      <th>Sweetness</th>\n",
              "      <th>Uniformity</th>\n",
              "      <th>Variety</th>\n",
              "      <th>altitude_high_meters</th>\n",
              "      <th>altitude_low_meters</th>\n",
              "      <th>altitude_mean_meters</th>\n",
              "      <th>quality_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>43</td>\n",
              "      <td>8.08</td>\n",
              "      <td>7.75</td>\n",
              "      <td>8.08</td>\n",
              "      <td>15 kg</td>\n",
              "      <td>7.83</td>\n",
              "      <td>7.75</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10.00</td>\n",
              "      <td>Green</td>\n",
              "      <td>taiwan coffee laboratory</td>\n",
              "      <td>Taiwan</td>\n",
              "      <td>8.42</td>\n",
              "      <td>June 9th, 2016</td>\n",
              "      <td>Sunshine Valley Estate 向陽高山咖啡</td>\n",
              "      <td>8.17</td>\n",
              "      <td>June 10th, 2015</td>\n",
              "      <td>2015</td>\n",
              "      <td>Taiwan</td>\n",
              "      <td>Specialty Coffee Association</td>\n",
              "      <td>sunshine valley estate 向陽高山咖啡</td>\n",
              "      <td>0.12</td>\n",
              "      <td>10</td>\n",
              "      <td>Semi-washed / Semi-pulped</td>\n",
              "      <td>LIN YEN CHIEN 林言謙</td>\n",
              "      <td>natou county</td>\n",
              "      <td>Arabica</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Bourbon</td>\n",
              "      <td>1000.00</td>\n",
              "      <td>1000.00</td>\n",
              "      <td>1000.00</td>\n",
              "      <td>86.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>425</td>\n",
              "      <td>7.50</td>\n",
              "      <td>7.50</td>\n",
              "      <td>7.67</td>\n",
              "      <td>15 kg</td>\n",
              "      <td>7.83</td>\n",
              "      <td>7.67</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10.00</td>\n",
              "      <td>Blue-Green</td>\n",
              "      <td>taiwan coffee laboratory</td>\n",
              "      <td>Taiwan</td>\n",
              "      <td>7.58</td>\n",
              "      <td>June 17th, 2016</td>\n",
              "      <td>Sunshine Valley Estate 向陽高山咖啡</td>\n",
              "      <td>7.50</td>\n",
              "      <td>June 18th, 2015</td>\n",
              "      <td>2015</td>\n",
              "      <td>Taiwan</td>\n",
              "      <td>Specialty Coffee Association</td>\n",
              "      <td>sunshine valley estate 向陽高山咖啡</td>\n",
              "      <td>0.10</td>\n",
              "      <td>10</td>\n",
              "      <td>Washed / Wet</td>\n",
              "      <td>LIN YEN CHIEN 林言謙</td>\n",
              "      <td>natou county</td>\n",
              "      <td>Arabica</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Bourbon</td>\n",
              "      <td>1000.00</td>\n",
              "      <td>1000.00</td>\n",
              "      <td>1000.00</td>\n",
              "      <td>83.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1249</th>\n",
              "      <td>1249</td>\n",
              "      <td>7.17</td>\n",
              "      <td>6.67</td>\n",
              "      <td>7.00</td>\n",
              "      <td>10 kg</td>\n",
              "      <td>6.83</td>\n",
              "      <td>6.83</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>9.33</td>\n",
              "      <td>Green</td>\n",
              "      <td>unex guatemala, s.a.</td>\n",
              "      <td>Taiwan</td>\n",
              "      <td>6.67</td>\n",
              "      <td>February 7th, 2014</td>\n",
              "      <td>Dongshan Gaoyuan village chief manor coffee Ta...</td>\n",
              "      <td>6.83</td>\n",
              "      <td>May 29th, 2013</td>\n",
              "      <td>2012</td>\n",
              "      <td>Taiwan</td>\n",
              "      <td>Specialty Coffee Association</td>\n",
              "      <td>beneficio ixchel</td>\n",
              "      <td>0.11</td>\n",
              "      <td>100</td>\n",
              "      <td>Washed / Wet</td>\n",
              "      <td>LUIS RODRIGUEZ</td>\n",
              "      <td>oriente</td>\n",
              "      <td>Arabica</td>\n",
              "      <td>10.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>Bourbon</td>\n",
              "      <td>1310.64</td>\n",
              "      <td>1310.64</td>\n",
              "      <td>1310.64</td>\n",
              "      <td>77.67</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0  Acidity  ...  altitude_mean_meters  quality_score\n",
              "43            43     8.08  ...               1000.00          86.08\n",
              "425          425     7.50  ...               1000.00          83.25\n",
              "1249        1249     7.17  ...               1310.64          77.67\n",
              "\n",
              "[3 rows x 35 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNP5QFN5yZEO",
        "colab_type": "text"
      },
      "source": [
        "### Scikit-learn Basics\n",
        "\n",
        "Scikit-learn is a great library to use for doing machine learning in Python. Data preparation, exploratory data analysis (EDA), classification, regression, clustering; it has it all. \n",
        "\n",
        "Scikit-learn usually expects data to be in the form of a 2D matrix with dimensions *n_samples x n_features* with an additional column for the target. To get acquainted with scikit-learn, we are going to use the [iris dataset](https://archive.ics.uci.edu/ml/datasets/iris), one of the most famous datasets in pattern recognition. \n",
        "\n",
        "Each entry in the dataset represents an iris plant, and is categorized as: \n",
        "\n",
        "* Setosa (class 0)\n",
        "* Versicolor (class 1)\n",
        "* Virginica (class 2)\n",
        "\n",
        "These represent the target classes to predict. Each entry also includes a set of features, namely:\n",
        "\n",
        "* Sepal width (cm)\n",
        "* Sepal length (cm)\n",
        "* Petal length (cm)\n",
        "* Petal width (cm)\n",
        "\n",
        "In the context of machine learning classification, the remainder of the lab is going to investigate the following question:  \n",
        "*Can we design a model that, based on the iris sample features, can accurately predict the iris sample class? *\n",
        "\n",
        "Scikit-learn has a copy of the iris dataset readily importable for us. Let's grab it now and conduct some EDA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nskz26U2yZEO",
        "colab_type": "code",
        "outputId": "1a57578c-faa5-4f8e-cae9-de4314ef6455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris_data = load_iris()\n",
        "feature_data = iris_data.data\n",
        "print(iris_data)\n",
        "np.shape(iris_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
            "       [4.9, 3. , 1.4, 0.2],\n",
            "       [4.7, 3.2, 1.3, 0.2],\n",
            "       [4.6, 3.1, 1.5, 0.2],\n",
            "       [5. , 3.6, 1.4, 0.2],\n",
            "       [5.4, 3.9, 1.7, 0.4],\n",
            "       [4.6, 3.4, 1.4, 0.3],\n",
            "       [5. , 3.4, 1.5, 0.2],\n",
            "       [4.4, 2.9, 1.4, 0.2],\n",
            "       [4.9, 3.1, 1.5, 0.1],\n",
            "       [5.4, 3.7, 1.5, 0.2],\n",
            "       [4.8, 3.4, 1.6, 0.2],\n",
            "       [4.8, 3. , 1.4, 0.1],\n",
            "       [4.3, 3. , 1.1, 0.1],\n",
            "       [5.8, 4. , 1.2, 0.2],\n",
            "       [5.7, 4.4, 1.5, 0.4],\n",
            "       [5.4, 3.9, 1.3, 0.4],\n",
            "       [5.1, 3.5, 1.4, 0.3],\n",
            "       [5.7, 3.8, 1.7, 0.3],\n",
            "       [5.1, 3.8, 1.5, 0.3],\n",
            "       [5.4, 3.4, 1.7, 0.2],\n",
            "       [5.1, 3.7, 1.5, 0.4],\n",
            "       [4.6, 3.6, 1. , 0.2],\n",
            "       [5.1, 3.3, 1.7, 0.5],\n",
            "       [4.8, 3.4, 1.9, 0.2],\n",
            "       [5. , 3. , 1.6, 0.2],\n",
            "       [5. , 3.4, 1.6, 0.4],\n",
            "       [5.2, 3.5, 1.5, 0.2],\n",
            "       [5.2, 3.4, 1.4, 0.2],\n",
            "       [4.7, 3.2, 1.6, 0.2],\n",
            "       [4.8, 3.1, 1.6, 0.2],\n",
            "       [5.4, 3.4, 1.5, 0.4],\n",
            "       [5.2, 4.1, 1.5, 0.1],\n",
            "       [5.5, 4.2, 1.4, 0.2],\n",
            "       [4.9, 3.1, 1.5, 0.2],\n",
            "       [5. , 3.2, 1.2, 0.2],\n",
            "       [5.5, 3.5, 1.3, 0.2],\n",
            "       [4.9, 3.6, 1.4, 0.1],\n",
            "       [4.4, 3. , 1.3, 0.2],\n",
            "       [5.1, 3.4, 1.5, 0.2],\n",
            "       [5. , 3.5, 1.3, 0.3],\n",
            "       [4.5, 2.3, 1.3, 0.3],\n",
            "       [4.4, 3.2, 1.3, 0.2],\n",
            "       [5. , 3.5, 1.6, 0.6],\n",
            "       [5.1, 3.8, 1.9, 0.4],\n",
            "       [4.8, 3. , 1.4, 0.3],\n",
            "       [5.1, 3.8, 1.6, 0.2],\n",
            "       [4.6, 3.2, 1.4, 0.2],\n",
            "       [5.3, 3.7, 1.5, 0.2],\n",
            "       [5. , 3.3, 1.4, 0.2],\n",
            "       [7. , 3.2, 4.7, 1.4],\n",
            "       [6.4, 3.2, 4.5, 1.5],\n",
            "       [6.9, 3.1, 4.9, 1.5],\n",
            "       [5.5, 2.3, 4. , 1.3],\n",
            "       [6.5, 2.8, 4.6, 1.5],\n",
            "       [5.7, 2.8, 4.5, 1.3],\n",
            "       [6.3, 3.3, 4.7, 1.6],\n",
            "       [4.9, 2.4, 3.3, 1. ],\n",
            "       [6.6, 2.9, 4.6, 1.3],\n",
            "       [5.2, 2.7, 3.9, 1.4],\n",
            "       [5. , 2. , 3.5, 1. ],\n",
            "       [5.9, 3. , 4.2, 1.5],\n",
            "       [6. , 2.2, 4. , 1. ],\n",
            "       [6.1, 2.9, 4.7, 1.4],\n",
            "       [5.6, 2.9, 3.6, 1.3],\n",
            "       [6.7, 3.1, 4.4, 1.4],\n",
            "       [5.6, 3. , 4.5, 1.5],\n",
            "       [5.8, 2.7, 4.1, 1. ],\n",
            "       [6.2, 2.2, 4.5, 1.5],\n",
            "       [5.6, 2.5, 3.9, 1.1],\n",
            "       [5.9, 3.2, 4.8, 1.8],\n",
            "       [6.1, 2.8, 4. , 1.3],\n",
            "       [6.3, 2.5, 4.9, 1.5],\n",
            "       [6.1, 2.8, 4.7, 1.2],\n",
            "       [6.4, 2.9, 4.3, 1.3],\n",
            "       [6.6, 3. , 4.4, 1.4],\n",
            "       [6.8, 2.8, 4.8, 1.4],\n",
            "       [6.7, 3. , 5. , 1.7],\n",
            "       [6. , 2.9, 4.5, 1.5],\n",
            "       [5.7, 2.6, 3.5, 1. ],\n",
            "       [5.5, 2.4, 3.8, 1.1],\n",
            "       [5.5, 2.4, 3.7, 1. ],\n",
            "       [5.8, 2.7, 3.9, 1.2],\n",
            "       [6. , 2.7, 5.1, 1.6],\n",
            "       [5.4, 3. , 4.5, 1.5],\n",
            "       [6. , 3.4, 4.5, 1.6],\n",
            "       [6.7, 3.1, 4.7, 1.5],\n",
            "       [6.3, 2.3, 4.4, 1.3],\n",
            "       [5.6, 3. , 4.1, 1.3],\n",
            "       [5.5, 2.5, 4. , 1.3],\n",
            "       [5.5, 2.6, 4.4, 1.2],\n",
            "       [6.1, 3. , 4.6, 1.4],\n",
            "       [5.8, 2.6, 4. , 1.2],\n",
            "       [5. , 2.3, 3.3, 1. ],\n",
            "       [5.6, 2.7, 4.2, 1.3],\n",
            "       [5.7, 3. , 4.2, 1.2],\n",
            "       [5.7, 2.9, 4.2, 1.3],\n",
            "       [6.2, 2.9, 4.3, 1.3],\n",
            "       [5.1, 2.5, 3. , 1.1],\n",
            "       [5.7, 2.8, 4.1, 1.3],\n",
            "       [6.3, 3.3, 6. , 2.5],\n",
            "       [5.8, 2.7, 5.1, 1.9],\n",
            "       [7.1, 3. , 5.9, 2.1],\n",
            "       [6.3, 2.9, 5.6, 1.8],\n",
            "       [6.5, 3. , 5.8, 2.2],\n",
            "       [7.6, 3. , 6.6, 2.1],\n",
            "       [4.9, 2.5, 4.5, 1.7],\n",
            "       [7.3, 2.9, 6.3, 1.8],\n",
            "       [6.7, 2.5, 5.8, 1.8],\n",
            "       [7.2, 3.6, 6.1, 2.5],\n",
            "       [6.5, 3.2, 5.1, 2. ],\n",
            "       [6.4, 2.7, 5.3, 1.9],\n",
            "       [6.8, 3. , 5.5, 2.1],\n",
            "       [5.7, 2.5, 5. , 2. ],\n",
            "       [5.8, 2.8, 5.1, 2.4],\n",
            "       [6.4, 3.2, 5.3, 2.3],\n",
            "       [6.5, 3. , 5.5, 1.8],\n",
            "       [7.7, 3.8, 6.7, 2.2],\n",
            "       [7.7, 2.6, 6.9, 2.3],\n",
            "       [6. , 2.2, 5. , 1.5],\n",
            "       [6.9, 3.2, 5.7, 2.3],\n",
            "       [5.6, 2.8, 4.9, 2. ],\n",
            "       [7.7, 2.8, 6.7, 2. ],\n",
            "       [6.3, 2.7, 4.9, 1.8],\n",
            "       [6.7, 3.3, 5.7, 2.1],\n",
            "       [7.2, 3.2, 6. , 1.8],\n",
            "       [6.2, 2.8, 4.8, 1.8],\n",
            "       [6.1, 3. , 4.9, 1.8],\n",
            "       [6.4, 2.8, 5.6, 2.1],\n",
            "       [7.2, 3. , 5.8, 1.6],\n",
            "       [7.4, 2.8, 6.1, 1.9],\n",
            "       [7.9, 3.8, 6.4, 2. ],\n",
            "       [6.4, 2.8, 5.6, 2.2],\n",
            "       [6.3, 2.8, 5.1, 1.5],\n",
            "       [6.1, 2.6, 5.6, 1.4],\n",
            "       [7.7, 3. , 6.1, 2.3],\n",
            "       [6.3, 3.4, 5.6, 2.4],\n",
            "       [6.4, 3.1, 5.5, 1.8],\n",
            "       [6. , 3. , 4.8, 1.8],\n",
            "       [6.9, 3.1, 5.4, 2.1],\n",
            "       [6.7, 3.1, 5.6, 2.4],\n",
            "       [6.9, 3.1, 5.1, 2.3],\n",
            "       [5.8, 2.7, 5.1, 1.9],\n",
            "       [6.8, 3.2, 5.9, 2.3],\n",
            "       [6.7, 3.3, 5.7, 2.5],\n",
            "       [6.7, 3. , 5.2, 2.3],\n",
            "       [6.3, 2.5, 5. , 1.9],\n",
            "       [6.5, 3. , 5.2, 2. ],\n",
            "       [6.2, 3.4, 5.4, 2.3],\n",
            "       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': '/usr/local/lib/python3.6/dist-packages/sklearn/datasets/data/iris.csv'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ2ZaTIQyZER",
        "colab_type": "text"
      },
      "source": [
        "**YOUR TURN:** \"feature_data\" now contains the feature data for all of the iris samples. \n",
        "* What is the shape of this feature data? **150,4**\n",
        "* The data type? **float 64**\n",
        "* How many samples are there? **150**\n",
        "* How many features are there? **4**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r53HzICUyZES",
        "colab_type": "code",
        "outputId": "a7ba5ef9-a775-44e3-cf9e-f82b79e9f419",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "## Enter your code her\n",
        "print(feature_data.shape)\n",
        "print(feature_data.dtype.name)\n",
        "print(len(feature_data))\n",
        "print(feature_data.shape[1])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150, 4)\n",
            "float64\n",
            "150\n",
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFtvXTKV3WUg",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9H6Z8NMyZEU",
        "colab_type": "text"
      },
      "source": [
        "Next, we will save the target classification data in a similar fashion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq1bSkNWyZEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_data = iris_data.target\n",
        "target_names = iris_data.target_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-UFVjmcyZEZ",
        "colab_type": "text"
      },
      "source": [
        "**YOUR TURN:**\n",
        "* What values are in \"target_data\"? **0,1,2**\n",
        "* What is the data type? **int64**\n",
        "* What values are in \"target_names\"? ['setosa' 'versicolor' 'virginica'] **bold text**\n",
        "* What is the data type? **str320**\n",
        "* How many samples are of type \"setosa\"?   \n",
        "    **50**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSot98BYyZEb",
        "colab_type": "code",
        "outputId": "0f3e4ca2-7121-41ac-e423-85eaa6aca449",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "## Enter your code here\n",
        "import numpy as np\n",
        "print(target_data)\n",
        "print(target_data.dtype)\n",
        "print(target_names)\n",
        "print(target_names.dtype.name)\n",
        "np.count_nonzero(target_data==0,axis=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n",
            "int64\n",
            "['setosa' 'versicolor' 'virginica']\n",
            "str320\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyBrtJttyZEk",
        "colab_type": "text"
      },
      "source": [
        "We can also do some more visual EDA by plotting the samples according to a subset of the features and coloring the data points to coincide with the sample classification. We will use [matplotlib](https://matplotlib.org/), a powerful plotting library within Python, to accomplish this.\n",
        "\n",
        "For example, lets plot sepal width vs. sepal length.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0Z3t7QoyZEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yptoDrGTyZEp",
        "colab_type": "code",
        "outputId": "684c288a-90ef-49d7-9845-5fd648051e41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "setosa = feature_data[target_data==0]\n",
        "versicolor = feature_data[target_data==1]\n",
        "virginica = feature_data[target_data==2]\n",
        "\n",
        "plt.scatter(setosa[:,0], setosa[:,1], label=\"setosa\")\n",
        "plt.scatter(versicolor[:,0], versicolor[:,1], label=\"versicolor\")\n",
        "plt.scatter(virginica[:,0], virginica[:,1], label=\"virginica\")\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel(\"sepal length (cm)\")\n",
        "plt.ylabel(\"sepal width (cm)\")\n",
        "plt.title(\"Visual EDA\");"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSFlhgC0yZEr",
        "colab_type": "text"
      },
      "source": [
        "In the above step, we used boolean indexing to filter the feature data based on the target data class. This allowed us to create a scatter plot for each of the iris classes and distinguish them by color.\n",
        "\n",
        "*Observations*: We can see that the \"setosa\" class typically consists of medium-to-high sepal width with low-to-medium sepal length, while the other two classes have lower width and higher length. The \"virginica\" class appears to have the largest combination of the two. \n",
        "\n",
        "**YOUR TURN:** \n",
        "* Which of the iris classes is seperable based on sepal characteristics? **setosa**\n",
        "* Which of the iris classes is not? **versicolor and virginica**\n",
        "* Can we (easily) visualize each of the samples w.r.t. all features on the same plot? Why/why not? **No. As the petals and sepals have same feature variables i.e length and width, plotting them on the same graph might not be the effective way to visualize data.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCvEGfrKyZEs",
        "colab_type": "text"
      },
      "source": [
        "### Creating a Nearest Neighbors Classifier\n",
        "\n",
        "Now that we've explored the data a little bit, we're going to use scikit-learn to create a nearest neighbors classifier for the data. Effectively we'll be developing a model whose job it is to build a relationship over input feature data (sepal and petal characteristics) that predicts the iris sample class (e.g. \"setosa\"). This is an example of a *supervised learning* task; we have all the features and all the target classes.\n",
        "\n",
        "Model creation in scikit-learn follows a **data prep -> fit -> predict** process. The \"fit\" function is where the actual model is trained and parameter values are selected, while the \"predict\" function actually takes the trained model and applies it to the new samples.\n",
        "\n",
        "First, we load the nearest neighbor library from scikit-learn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA1cHqdpyZEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import neighbors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64U6z3mIyZEv",
        "colab_type": "text"
      },
      "source": [
        "Now, we're going to save our feature data into an array called 'X' and our target data into an array called 'y'. We don't *need* to do this, but it is traditional to think of the problem using this notation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLaSzmgOyZEv",
        "colab_type": "code",
        "outputId": "27e0d92f-57ee-42f7-da70-4b9bb7951c83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X = feature_data\n",
        "y = target_data\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.4 3.7 1.5 0.2]\n",
            " [4.8 3.4 1.6 0.2]\n",
            " [4.8 3.  1.4 0.1]\n",
            " [4.3 3.  1.1 0.1]\n",
            " [5.8 4.  1.2 0.2]\n",
            " [5.7 4.4 1.5 0.4]\n",
            " [5.4 3.9 1.3 0.4]\n",
            " [5.1 3.5 1.4 0.3]\n",
            " [5.7 3.8 1.7 0.3]\n",
            " [5.1 3.8 1.5 0.3]\n",
            " [5.4 3.4 1.7 0.2]\n",
            " [5.1 3.7 1.5 0.4]\n",
            " [4.6 3.6 1.  0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.  3.  1.6 0.2]\n",
            " [5.  3.4 1.6 0.4]\n",
            " [5.2 3.5 1.5 0.2]\n",
            " [5.2 3.4 1.4 0.2]\n",
            " [4.7 3.2 1.6 0.2]\n",
            " [4.8 3.1 1.6 0.2]\n",
            " [5.4 3.4 1.5 0.4]\n",
            " [5.2 4.1 1.5 0.1]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.2]\n",
            " [5.  3.2 1.2 0.2]\n",
            " [5.5 3.5 1.3 0.2]\n",
            " [4.9 3.6 1.4 0.1]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.1 3.4 1.5 0.2]\n",
            " [5.  3.5 1.3 0.3]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [5.  3.5 1.6 0.6]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [4.8 3.  1.4 0.3]\n",
            " [5.1 3.8 1.6 0.2]\n",
            " [4.6 3.2 1.4 0.2]\n",
            " [5.3 3.7 1.5 0.2]\n",
            " [5.  3.3 1.4 0.2]\n",
            " [7.  3.2 4.7 1.4]\n",
            " [6.4 3.2 4.5 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [5.5 2.3 4.  1.3]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [5.7 2.8 4.5 1.3]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [4.9 2.4 3.3 1. ]\n",
            " [6.6 2.9 4.6 1.3]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [5.  2.  3.5 1. ]\n",
            " [5.9 3.  4.2 1.5]\n",
            " [6.  2.2 4.  1. ]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [5.6 2.9 3.6 1.3]\n",
            " [6.7 3.1 4.4 1.4]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.8 2.7 4.1 1. ]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.6 2.5 3.9 1.1]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.1 2.8 4.  1.3]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.8 4.7 1.2]\n",
            " [6.4 2.9 4.3 1.3]\n",
            " [6.6 3.  4.4 1.4]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.7 3.  5.  1.7]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.7 2.6 3.5 1. ]\n",
            " [5.5 2.4 3.8 1.1]\n",
            " [5.5 2.4 3.7 1. ]\n",
            " [5.8 2.7 3.9 1.2]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.  3.4 4.5 1.6]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [5.5 2.5 4.  1.3]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [6.1 3.  4.6 1.4]\n",
            " [5.8 2.6 4.  1.2]\n",
            " [5.  2.3 3.3 1. ]\n",
            " [5.6 2.7 4.2 1.3]\n",
            " [5.7 3.  4.2 1.2]\n",
            " [5.7 2.9 4.2 1.3]\n",
            " [6.2 2.9 4.3 1.3]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [5.7 2.8 4.1 1.3]\n",
            " [6.3 3.3 6.  2.5]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [7.1 3.  5.9 2.1]\n",
            " [6.3 2.9 5.6 1.8]\n",
            " [6.5 3.  5.8 2.2]\n",
            " [7.6 3.  6.6 2.1]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [7.3 2.9 6.3 1.8]\n",
            " [6.7 2.5 5.8 1.8]\n",
            " [7.2 3.6 6.1 2.5]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.4 2.7 5.3 1.9]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [5.7 2.5 5.  2. ]\n",
            " [5.8 2.8 5.1 2.4]\n",
            " [6.4 3.2 5.3 2.3]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [7.7 3.8 6.7 2.2]\n",
            " [7.7 2.6 6.9 2.3]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.9 3.2 5.7 2.3]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [7.7 2.8 6.7 2. ]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.7 3.3 5.7 2.1]\n",
            " [7.2 3.2 6.  1.8]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [6.4 2.8 5.6 2.1]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [7.4 2.8 6.1 1.9]\n",
            " [7.9 3.8 6.4 2. ]\n",
            " [6.4 2.8 5.6 2.2]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.1 2.6 5.6 1.4]\n",
            " [7.7 3.  6.1 2.3]\n",
            " [6.3 3.4 5.6 2.4]\n",
            " [6.4 3.1 5.5 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.9 3.1 5.4 2.1]\n",
            " [6.7 3.1 5.6 2.4]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [6.8 3.2 5.9 2.3]\n",
            " [6.7 3.3 5.7 2.5]\n",
            " [6.7 3.  5.2 2.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [6.2 3.4 5.4 2.3]\n",
            " [5.9 3.  5.1 1.8]]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_1B6hKlyZEy",
        "colab_type": "text"
      },
      "source": [
        "Next, we create our nearest neighbor classifier object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1yoeBg_yZEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "knn = neighbors.KNeighborsClassifier(n_neighbors=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE3I_-ggyZE7",
        "colab_type": "text"
      },
      "source": [
        "And then we *fit* it to the data (i.e., train the classifier)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyI5TqiWyZE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "knn.fit(X,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DagP3Wc_yZFC",
        "colab_type": "text"
      },
      "source": [
        " Now we have a model! If you're new to this, you've officially built your first machine learning model. If you use \"knn.predict(*[[feature array here]]*)\", you can use your trained model to predict the class of a new iris sample. \n",
        "\n",
        "**YOUR TURN:**\n",
        "* What is the predicted class of a new iris sample with feature vector [3,4,5,2]? \n",
        "What is its name? array[2] **virginica**\n",
        "* Do you think this model is overfit or underfit to the iris dataset? Why? \n",
        "  **Underfit as the data was not split into training,validation and test sets**\n",
        "* How many neighbors does our model consider when classifying a new sample? **1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4nr0LuLyZFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "knn.predict([[3,4,5,2]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tipfbx0fyZFF",
        "colab_type": "text"
      },
      "source": [
        "As you may have noted in the previous cell, we've trained this classifier on our *entire dataset*. This typically isn't done in practice and results in overfitting to the data. Here's a bit of a tricky question:\n",
        "\n",
        "**YOUR TURN:**\n",
        "* If we use our classifier to predict the classes of the iris samples that were used to train the model itself, what will our overall accuracy be?: **100%**\n",
        "\n",
        "We can validate our hypothesis fairly easily using either: i) the NumPy technique for calculating accuracy we used earlier in the lab, or ii) scikit-learn's in-house \"accuracy_score()\" function.\n",
        "\n",
        "Let's use our technique first:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPMfcyB4yZFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "accuracy = np.sum(target_data == knn.predict(feature_data)) / target_data.size\n",
        "print (\"Accuracy: \", accuracy * 100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djYGa2juyZFK",
        "colab_type": "text"
      },
      "source": [
        "and then using scikit-learn's customized function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OfJOT3TyZFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(target_data, knn.predict(feature_data))\n",
        "print (\"Accuracy: \", accuracy * 100, \"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJm79bvoyZFO",
        "colab_type": "text"
      },
      "source": [
        "We see that our classifier has achieved 100% accuracy (and both calculation methods agree)!\n",
        "\n",
        "**DISCUSSION:** \n",
        "* Why do you think the model was able to achieve such a \"great\" result? The test data and the trained data are similar\n",
        "* What does this really tell us? When trained data and test data are similar we achieve 100% accuracy\n",
        "* Do you expect the model to perform this well on new data? No"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYlo7mIXyZFe",
        "colab_type": "text"
      },
      "source": [
        "## Exercise (to be completed on your own)\n",
        "\n",
        "Let's take the tools we have learned in this lab and put them into practice on a new dataset.\n",
        "\n",
        "We're going to work with a dataset focused on diabetes. It contains a variety of health metrics for a number of patients, and then in a second object it shows whether or not that patient had diabetes. Download it using the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFo8KVcryZFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "diabetes_data = fetch_openml(\n",
        "    name='diabetes',\n",
        "    cache=False\n",
        ")\n",
        "diabetes_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOGmwyQNyZFh",
        "colab_type": "text"
      },
      "source": [
        "First off, take a look at the `data`, `target` and `feature_names` entires in the `diabetes_data` dictionary. They contain the information we'll be working with here. Then, create a Pandas DataFrame called `diabetes_df` containing the data and the targets, with the feature names as column headings. If you need help, refer [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) for more detail on how to achieve this.\n",
        "\n",
        "* What was the average age of participants? **33.240885** [0.5]\n",
        "* How many participants tested positive? How many tested negative?  **268, 500** [0.5]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nffHJ6yGCqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE\n",
        "import pandas as pd\n",
        "diabetes_df = pd.DataFrame(diabetes_data.data , columns=diabetes_data.feature_names)\n",
        "diabetes_df['target']=diabetes_data.target\n",
        "print(diabetes_df)\n",
        "age=diabetes_df['age'].mean()\n",
        "print(diabetes_df['target'].value_counts())\n",
        "print(age)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27UvyCPUyZFq",
        "colab_type": "text"
      },
      "source": [
        "The targets are currently a string representing whether or not the patient has diabetes. However, it's more useful for us if this column contains a 1 or a 0 depending on whether the patient has diabetes. Use the [Label Encoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) class from Scikit-Learn to convert the labels into integers. [0.5]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WNvJzdsyZFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "diabetes_df['target']=le.fit_transform(diabetes_df['target'])\n",
        "diabetes_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBmo0-W1yZFs",
        "colab_type": "text"
      },
      "source": [
        "Now we are going to create a classifier to predict whether a patient has diabetes based on their vitals. \n",
        "\n",
        " **Note**:\n",
        " \n",
        " We should have a training, validation and test set and our main aim is to get the best accuracy on test set. While perfoming the cross-validation, the validation set is selected by the validation function each time, the input to that function should be all of dataset except your test set. \n",
        "\n",
        "So, a general guideline to follow will be first split the dataset into train and test (70:30), after this keep the test set aside untouched for final evaluation.\n",
        "Further, in the training set depending upon the number of folds for cross validation a validation set can be obtained, the model is then iteratively trained and validated on these different sets. Basically you use your training set to generate multiple splits of the Train and Validation sets.\n",
        "\n",
        "* Using `cross_val_score`, report mean cross validation accuracy on a KNN classifier with K=3 and 10 folds. Remember that the `target` column holds our labels\n",
        "\n",
        "  **0.7030** [0.5] \n",
        "\n",
        "For all the cases mentioned below test accuracy should be computed and reported:\n",
        "\n",
        "* What accuracy did the model achieve on test set? \n",
        "   **67.099%**[1]\n",
        "* Find a value for K that performs better than this (Note: Try for atleast first 50 values of k). \n",
        "\n",
        "What value for K did you use?**k= 17**\n",
        "What was the performance **The accuracy curve was declining as the number of folds were increased**? (Note: Show some Visual Depiction to compare Model performance)**The visual description was shown at the end of knn classifier session**[2]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsGm3b00yZFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import neighbors\n",
        "X=diabetes_df\n",
        "y = diabetes_df.target\n",
        "from sklearn.model_selection import train_test_split\n",
        "#split dataset into train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DelgzpNEWCrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create KNN classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# Create KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors = 3)\n",
        "# Fit the classifier to the data\n",
        "knn.fit(X_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIxPsgivWNUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#show first 5 model predictions on the test data\n",
        "knn.predict(X_test)[0:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c86MlEOeWALJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accu=knn.score(X_test, y_test)*100\n",
        "print(\"Accuracy=\",accu,\"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEasRT7TWp_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "#create a new KNN model with k=3\n",
        "knn_cv = KNeighborsClassifier(n_neighbors=3)\n",
        "#train model with cv of 10 \n",
        "cv_scores = cross_val_score(knn_cv, X, y, cv=10)\n",
        "#print each cv score (accuracy) and average them\n",
        "print(cv_scores)\n",
        "print(\"average=\" ,np.mean(cv_scores))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFGnwSvDWWMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "#create a new KNN model\n",
        "var=[]\n",
        "for i in range(1,51):\n",
        "  knn_cv = KNeighborsClassifier(n_neighbors=i)\n",
        "#train model with cv of 10 \n",
        "  cv_scores = cross_val_score(knn_cv, X, y, cv=10)\n",
        "#print each cv score (accuracy) and average them\n",
        "  print(\"average[\",i,\"]:\" ,np.mean(cv_scores))\n",
        "  var.append(np.mean(cv_scores))\n",
        "print(\"Max Average CV Score:\",(max(var)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr1HryANJ2ZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "training_accuracy = []\n",
        "test_accuracy = []\n",
        "# try n_neighbors from 1 to 50\n",
        "neighbors_settings = range(1, 51)\n",
        "for n_neighbors in neighbors_settings:\n",
        "    # build the model\n",
        "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
        "    knn.fit(X_train, y_train)\n",
        "    # record training set accuracy\n",
        "    training_accuracy.append(knn.score(X_train, y_train))\n",
        "    # record test set accuracy\n",
        "    test_accuracy.append(knn.score(X_test, y_test))\n",
        "print(\"Max Test accuracy = \", np.mean(test_accuracy)*100)\n",
        "print(\"Max Training accuracy = \", np.mean(training_accuracy)*100)\n",
        "plt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\n",
        "plt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"n_neighbors\")\n",
        "plt.legend()\n",
        "plt.savefig('knn_compare_model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp9EDL-FyZFv",
        "colab_type": "text"
      },
      "source": [
        "Take a look at the `skin` feature.\n",
        "\n",
        "* According to the dataset description in `diabetes_data['DESCR']`, what does this feature represent? **This feature gives information about the authors,source,citations and an overview of the diabetes data in the imported file.**  [0.25]\n",
        "* Are there any unusual entries in this column? If so, why? [0.25] **The data consists of zero values in the skin and insu feature**\n",
        "\n",
        "Use the `SimpleImputer` class from scikit-learn to impute missing values for the `skin` and `insu` columns. Overwrite the existing `skin` and `insu` columns with these new values.[1]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JEhDq5lyZFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERETake a look at the skin feature.\n",
        "print(diabetes_df.skin,diabetes_df.insu)\n",
        "\n",
        "\n",
        "print(diabetes_data['DESCR'])\n",
        "#\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTzLzYCTcuDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### YOUR CODE HERE\n",
        "diabetes_data['DESCR']\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imp = SimpleImputer(\n",
        "    missing_values=0,\n",
        "    strategy='mean',\n",
        "    verbose=1 )\n",
        "\n",
        "imp.fit(\n",
        "    diabetes_df.skin.values.reshape((-1,1)) #we have to do the reshape operation because we are only using one feature.\n",
        ")\n",
        "\n",
        "diabetes_df.skin = imp.transform(diabetes_df.skin.values.reshape((-1,1)))\n",
        "\n",
        "\n",
        "imp1 = SimpleImputer(\n",
        "    missing_values=0,\n",
        "    strategy='mean',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "imp1.fit(\n",
        "    diabetes_df.insu.values.reshape((-1,1)) #we have to do the reshape operation because we are only using one feature.\n",
        ")\n",
        "\n",
        "diabetes_df.insu = imp1.transform(diabetes_df.insu.values.reshape((-1,1)))\n",
        "\n",
        "\n",
        "diabetes_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji8wI48ZyZFy",
        "colab_type": "text"
      },
      "source": [
        "Re-split the data and fit a new classifier. [2.5]\n",
        "\n",
        "\n",
        "Note: Test accuracy should be computed and reported below\n",
        "* Is performance better or worse with imputed values? Why might this be? (Note: Show some Visual Depiction to compare Model performance) **The accuracy increased to 72.72% and the cross validation score was changed to 0.7539986329460014 with k=18**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgCWLMF3yZFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import neighbors\n",
        "# Create KNN classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "X=diabetes_df\n",
        "y = diabetes_df.target\n",
        "from sklearn.model_selection import train_test_split\n",
        "#split dataset into train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n",
        "\n",
        "\n",
        "# Create KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors = 3)\n",
        "# Fit the classifier to the data\n",
        "knn.fit(X_train,y_train)\n",
        "knn.predict(X_test)[0:5]\n",
        "\n",
        "\n",
        "#cv scores\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "#cv for k=1 to 5 \n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "#create a new KNN model\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "#create a new KNN model\n",
        "var=[]\n",
        "for i in range(1,51):\n",
        "  knn_cv = KNeighborsClassifier(n_neighbors=i)\n",
        "#train model with cv of 10 \n",
        "  cv_scores = cross_val_score(knn_cv, X, y, cv=10)\n",
        "#print each cv score (accuracy) and average them\n",
        "  print(\"average[\",i,\"]:\" ,np.mean(cv_scores))\n",
        "  var.append(np.mean(cv_scores))\n",
        "print(\"Max Average CV Score:\",(max(var)))\n",
        "#visual representation\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "training_accuracy = []\n",
        "test_accuracy = []\n",
        "# try n_neighbors from 1 to 50\n",
        "neighbors_settings = range(1, 51)\n",
        "for n_neighbors in neighbors_settings:\n",
        "    # build the model\n",
        "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
        "    knn.fit(X_train, y_train)\n",
        "    # record training set accuracy\n",
        "    training_accuracy.append(knn.score(X_train, y_train))\n",
        "    # record test set accuracy\n",
        "    test_accuracy.append(knn.score(X_test, y_test))\n",
        "accu=knn.score(X_test, y_test)*100\n",
        "print(\"Accuracy=\",accu,\"%\")\n",
        "#plt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\n",
        "plt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"n_neighbors\")\n",
        "plt.legend()\n",
        "plt.savefig('knn_compare_model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Yryczkdc6nL",
        "colab_type": "text"
      },
      "source": [
        "References\n",
        "\n",
        "Train, Test and Validation set:\n",
        "\n",
        "https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7\n",
        "\n"
      ]
    }
  ]
}